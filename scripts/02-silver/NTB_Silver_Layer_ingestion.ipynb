{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"customers\", \"suffix\":\"gfs\", \"partion_column\": \"Date_Of_Birth\" },\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"discounts\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"employees\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"products\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"stores\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"transactions\", \"suffix\":\"gfs\", \"partion_column\": \"Date\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\miniconda3\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, col, lit\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"silver_layer.log\"\n",
    "\n",
    "# Remove any existing handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    filemode=\"w\",  # Overwrites log file on each run\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "logging.info(\"Logging system initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spark session with hive support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting Spark Session\")\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Silver_Layer_Ingesion\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "logging.info(\"Spark Session started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating function to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(source, file_name, partition_column = None):\n",
    "        try:\n",
    "                logs ={}\n",
    "                basepath = \"hdfs://0.0.0.0:19000/bronze_layer\"\n",
    "                dataset_path = basepath +\"/\"+ source + \"/\" + file_name + \"/\"\n",
    "                print(dataset_path)\n",
    "                data = spark.read\\\n",
    "                        .option(\"header\",\"true\")\\\n",
    "                        .option(\"inferSchema\",\"true\")\\\n",
    "                        .csv(dataset_path)\n",
    "\n",
    "                #remving duplicates\n",
    "                data = data.distinct()\n",
    "                \n",
    "                #renaming column\n",
    "                data = data.select([col(c).alias(c.replace(\" \", \"_\")) for c in data.columns])\n",
    "\n",
    "                # adding partitioning column\n",
    "                if partition_column != None:\n",
    "                        data = data.withColumn(\"year\", year(col(partition_column)))\n",
    "                else:\n",
    "                        current_year = datetime.now().year\n",
    "                        data = data.withColumn(\"year\", lit(current_year))\n",
    "                logs[\"dataset_path\"] = dataset_path\n",
    "                logs[\"file_name\"] = file_name\n",
    "                logs[\"no_of_rows\"] = data.count()\n",
    "                print(logs)\n",
    "                logging.info(f\"logs: {logs}\")\n",
    "                return data\n",
    "        except Exception as e:\n",
    "                logging.error(f\"Error cleaning data: {str(e)}\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating function to ingest data into hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_layer_ingestion(data, table_name, suffix):\n",
    "    try:\n",
    "        database = \"default\"  # Change if needed\n",
    "        table = suffix + \"_\" + table_name\n",
    "\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database}\")\n",
    "        spark.sql(f\"USE {database}\")\n",
    "\n",
    "        logging.info(f\"started loading {table_name} data to hive\")\n",
    "        # Save the DataFrame to Hive as a table\n",
    "        data.write.mode(\"overwrite\").partitionBy(\"year\").format(\"parquet\").saveAsTable(f\"{database}.{table}\")\n",
    "        logging.info(f\"{table_name} has successfully loaded\")\n",
    "    except Exception as e:\n",
    "                logging.error(f\"Error loading data to hive: {str(e)}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://0.0.0.0:19000/bronze_layer/global_fashion_sales/transactions/\n",
      "6416029\n",
      "DataFrame saved successfully in Hive!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(parameters)):\n",
    "    source = parameters[i].get('source')\n",
    "    table_name = parameters[i].get(\"table_name\")\n",
    "    suffix = parameters[i].get(\"suffix\")\n",
    "    partition_column = parameters[i].get(\"partion_column\", None)\n",
    "    data = cleaning_data(source,table_name,partition_column)\n",
    "    silver_layer_ingestion(data, table_name, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Stopping spark session\")\n",
    "spark.stop()\n",
    "logging(\"Spark Session stop, Job has been completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
