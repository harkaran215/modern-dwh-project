{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"customers\", \"suffix\":\"gfs\", \"partion_column\": \"Date_Of_Birth\" },\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"discounts\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"employees\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"products\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"stores\", \"suffix\":\"gfs\"},\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"transactions\", \"suffix\":\"gfs\", \"partion_column\": \"Date\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"source\":\"global_fashion_sales\", \"table_name\":\"transactions\", \"suffix\":\"gfs\", \"partion_column\": \"Date\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\miniconda3\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, col, lit\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"silver_layer.log\"\n",
    "\n",
    "# Remove any existing handlers\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    filemode=\"w\",  # Overwrites log file on each run\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "logging.info(\"Logging system initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Starting Spark Session\")\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Silver_Layer_Ingesion\")\\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\")\\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "logging.info(\"Spark Session started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(source, file_name, partition_column = None):\n",
    "        try:\n",
    "                logs ={}\n",
    "                basepath = \"hdfs://0.0.0.0:19000/bronze_layer\"\n",
    "                dataset_path = basepath +\"/\"+ source + \"/\" + file_name + \"/\"\n",
    "                print(dataset_path)\n",
    "                data = spark.read\\\n",
    "                        .option(\"header\",\"true\")\\\n",
    "                        .option(\"inferSchema\",\"true\")\\\n",
    "                        .csv(dataset_path)\n",
    "\n",
    "                #remving duplicates\n",
    "                data = data.distinct()\n",
    "                \n",
    "                #renaming column\n",
    "                data = data.select([col(c).alias(c.replace(\" \", \"_\")) for c in data.columns])\n",
    "\n",
    "                # adding partitioning column\n",
    "                if partition_column != None:\n",
    "                        data = data.withColumn(\"year\", year(col(partition_column)))\n",
    "                else:\n",
    "                        current_year = datetime.now().year\n",
    "                        data = data.withColumn(\"year\", lit(current_year))\n",
    "                logs[\"dataset_path\"] = dataset_path\n",
    "                logs[\"file_name\"] = file_name\n",
    "                logs[\"no_of_rows\"] = data.count()\n",
    "                print(logs)\n",
    "                logging.info(f\"logs: {logs}\")\n",
    "                return data\n",
    "        except Exception as e:\n",
    "                logging.error(f\"Error cleaning data: {str(e)}\", exc_info=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silver_layer_ingestion(data, table_name, suffix):\n",
    "    try:\n",
    "        database = \"default\"  # Change if needed\n",
    "        table = suffix + \"_\" + table_name\n",
    "\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database}\")\n",
    "        spark.sql(f\"USE {database}\")\n",
    "\n",
    "        logging.info(f\"started loading {table_name} data to hive\")\n",
    "        # Save the DataFrame to Hive as a table\n",
    "        data.write.mode(\"overwrite\").partitionBy(\"year\").format(\"parquet\").saveAsTable(f\"{database}.{table}\")\n",
    "        logging.info(f\"{table_name} has successfully loaded\")\n",
    "    except Exception as e:\n",
    "                logging.error(f\"Error loading data to hive: {str(e)}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://0.0.0.0:19000/bronze_layer/global_fashion_sales/transactions/\n",
      "6416029\n",
      "DataFrame saved successfully in Hive!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(parameters)):\n",
    "    source = parameters[i].get('source')\n",
    "    table_name = parameters[i].get(\"table_name\")\n",
    "    suffix = parameters[i].get(\"suffix\")\n",
    "    partition_column = parameters[i].get(\"partion_column\", None)\n",
    "    data = cleaning_data(source,table_name,partition_column)\n",
    "    silver_layer_ingestion(data, table_name, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Stopping spark session\")\n",
    "spark.stop()\n",
    "logging(\"Spark Session stop, Job has been completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+----------+----+---------+----------+--------+-------------------+--------+----------+--------+-----------+--------+---------------+--------------------+----------------+--------------+-------------+----+\n",
      "|         Invoice_ID|Line|Customer_ID|Product_ID|Size|    Color|Unit_Price|Quantity|               Date|Discount|Line_Total|Store_ID|Employee_ID|Currency|Currency_Symbol|                 SKU|Transaction_Type|Payment_Method|Invoice_Total|year|\n",
      "+-------------------+----+-----------+----------+----+---------+----------+--------+-------------------+--------+----------+--------+-----------+--------+---------------+--------------------+----------------+--------------+-------------+----+\n",
      "|INV-US-001-03764542|   6|      73817|      9258|   S|    WHITE|      35.5|       1|2024-07-28 20:51:00|     0.0|      35.5|       1|         10|     USD|              $|    FESW9258-S-WHITE|            Sale|          Cash|        361.0|2024|\n",
      "|INV-US-001-03745464|   1|      69219|     11530|   M|    LILAC|      85.0|       1|2024-05-11 11:31:00|     0.0|      85.0|       1|          8|     USD|              $|   MACO11530-M-LILAC|            Sale|   Credit Card|         85.0|2024|\n",
      "|INV-US-001-03775329|   1|      31018|     13118|   M|    GREEN|      49.0|       1|2024-09-05 18:37:00|     0.0|      49.0|       1|          6|     USD|              $|   MAT-13118-M-GREEN|            Sale|   Credit Card|         49.0|2024|\n",
      "|INV-US-001-03825438|   2|      16418|     13991|   S|     PINK|      48.5|       1|2024-12-21 20:33:00|     0.5|     24.25|       1|         13|     USD|              $|    FEDR13991-S-PINK|            Sale|   Credit Card|        200.5|2024|\n",
      "|INV-US-002-01960489|   3|     246482|     10388|   M|TURQUOISE|      21.0|       1|2024-04-01 08:37:00|     0.0|      21.0|       2|         21|     USD|              $|MAT-10388-M-TURQU...|            Sale|   Credit Card|        152.5|2024|\n",
      "|INV-US-002-02024796|   1|     108098|     14302|   S| BURGUNDY|      56.0|       3|2024-10-31 10:04:00|     0.0|     168.0|       2|         23|     USD|              $|FESW14302-S-BURGUNDY|            Sale|   Credit Card|        168.0|2024|\n",
      "|INV-US-002-02009128|   5|     101067|     14270|   M|    WHITE|      65.0|       1|2024-09-24 14:36:00|     0.0|      65.0|       2|         20|     USD|              $|   CHGI14270-M-WHITE|            Sale|   Credit Card|        636.0|2024|\n",
      "|RET-US-002-02021094|   2|      92556|     14243|   G|  MUSTARD|      36.5|       1|2024-10-22 00:00:00|     0.0|     -36.5|       2|         20|     USD|              $| CHBA14243-G-MUSTARD|          Return|   Credit Card|       -175.0|2024|\n",
      "|INV-US-002-01990629|   1|     121428|      9570|   M|TURQUOISE|      53.5|       1|2024-08-07 08:09:00|     0.0|      53.5|       2|         18|     USD|              $|FESW9570-M-TURQUOISE|            Sale|   Credit Card|         79.0|2024|\n",
      "|INV-US-005-04312095|   1|     282279|     10257|   M|    BLACK|      48.0|       1|2024-04-04 08:59:00|     0.0|      48.0|       5|         60|     USD|              $|   MASW10257-M-BLACK|            Sale|   Credit Card|        342.5|2024|\n",
      "|INV-US-004-01397591|   1|     212047|      9814|   L|    BEIGE|      56.5|       1|2024-03-31 19:34:00|     0.0|      56.5|       4|         43|     USD|              $|    MACO9814-L-BEIGE|            Sale|          Cash|         56.5|2024|\n",
      "|INV-US-003-04008139|   3|     234965|     13158|   M|  NEUTRAL|      69.5|       1|2024-12-14 15:10:00|     0.0|      69.5|       3|         36|     USD|              $| FESW13158-M-NEUTRAL|            Sale|          Cash|        180.5|2024|\n",
      "|INV-US-005-04335623|   1|     299628|     13612|   M|    BEIGE|      21.0|       1|2024-10-12 17:40:00|     0.0|      21.0|       5|         59|     USD|              $|   MAT-13612-M-BEIGE|            Sale|   Credit Card|         21.0|2024|\n",
      "|INV-CN-007-02190301|   1|     456007|      8765|   S|   YELLOW|     434.5|       1|2024-01-04 09:51:00|     0.0|     434.5|       7|         82|     CNY|              ¥|   FEDR8765-S-YELLOW|            Sale|   Credit Card|        644.8|2024|\n",
      "|INV-CN-006-03002162|   1|     380275|      9773|   P|  NEUTRAL|     582.0|       1|2024-04-26 13:52:00|     0.0|     582.0|       6|         71|     CNY|              ¥|  CHCO9773-P-NEUTRAL|            Sale|   Credit Card|        582.0|2024|\n",
      "|INV-CN-006-02992296|   7|     385704|      9996|   M|     PINK|     499.0|       2|2024-03-24 20:48:00|     0.0|     998.0|       6|         68|     CNY|              ¥|     MACO9996-M-PINK|            Sale|   Credit Card|       2718.0|2024|\n",
      "|INV-CN-006-03079715|   3|     364331|     13974|   M|     GOLD|     812.0|       1|2024-12-21 18:49:00|     0.5|     406.0|       6|         68|     CNY|              ¥|    MACO13974-M-GOLD|            Sale|   Credit Card|      1134.75|2024|\n",
      "|INV-CN-006-02971630|   1|     387982|      8135|   G|    BLACK|     281.5|       1|2024-01-06 19:30:00|     0.4|     168.9|       6|         70|     CNY|              ¥|    CHCO8135-G-BLACK|            Sale|   Credit Card|        309.0|2024|\n",
      "|INV-CN-007-02284654|   1|     453367|     14197|   S|TURQUOISE|     499.5|       1|2024-11-16 20:57:00|     0.0|     499.5|       7|         84|     CNY|              ¥|FECO14197-S-TURQU...|            Sale|   Credit Card|        499.5|2024|\n",
      "|RET-CN-007-02239659|   1|     525249|      9426|   L|     BLUE|     389.5|       1|2024-06-23 00:00:00|     0.0|    -389.5|       7|         83|     CNY|              ¥|     MAT-9426-L-BLUE|          Return|   Credit Card|       -389.5|2024|\n",
      "+-------------------+----+-----------+----------+----+---------+----------+--------+-------------------+--------+----------+--------+-----------+--------+---------------+--------------------+----------------+--------------+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from gfs_transactions\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"default\"  # Change if needed\n",
    "table = \"customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved successfully in Hive!\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database}\")\n",
    "spark.sql(f\"USE {database}\")\n",
    "\n",
    "# Save the DataFrame to Hive as a table\n",
    "df.write.mode(\"overwrite\").partitionBy(\"year\").format(\"parquet\").saveAsTable(f\"{database}.{table}\")\n",
    "\n",
    "print(\"DataFrame saved successfully in Hive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  default|customers|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----------------------+-----------+----+-------+------+-------------+---------+----+\n",
      "|Customer_ID|  Name|                  Email|  Telephone|City|Country|Gender|Date_Of_Birth|Job_Title|year|\n",
      "+-----------+------+-----------------------+-----------+----+-------+------+-------------+---------+----+\n",
      "|     600985|安雪梅|  安雪梅@fake_gmail.com|18884336961|惠州|   中国|     M|   2003-03-29|     NULL|2003|\n",
      "|     435522|  李娟|    李娟@fake_yahoo.com|13846999218|北京|   中国|     M|   2003-01-19|     NULL|2003|\n",
      "|     600992|  娄龙|    娄龙@fake_yahoo.com|15967115592|惠州|   中国|     M|   2003-04-03|     NULL|2003|\n",
      "|     435533|杨春梅|杨春梅@fake_hotmail.com|13952890708|北京|   中国|     F|   2003-12-03|     NULL|2003|\n",
      "|     601017|  李颖|  李颖@fake_hotmail.com|15548597618|惠州|   中国|     F|   2003-08-07|     NULL|2003|\n",
      "|     435548|徐小红|  徐小红@fake_gmail.com|14757181563|北京|   中国|     M|   2003-06-10|     NULL|2003|\n",
      "|     601018|  孙玉|    孙玉@fake_gmail.com|14789406324|惠州|   中国|     F|   2003-08-10|     NULL|2003|\n",
      "|     435559|  任琳|    任琳@fake_yahoo.com|13576570803|北京|   中国|     M|   2003-02-23|     NULL|2003|\n",
      "|     601022|李秀珍|  李秀珍@fake_gmail.com|18160948593|惠州|   中国|     M|   2003-11-06|     NULL|2003|\n",
      "|     435563|姚建平|姚建平@fake_hotmail.com|15523099036|北京|   中国|     F|   2003-06-13|     NULL|2003|\n",
      "|     601025|  蔡霞|    蔡霞@fake_yahoo.com|13680574941|惠州|   中国|     M|   2003-07-12|     NULL|2003|\n",
      "|     435566|  王欢|    王欢@fake_yahoo.com|13559352228|北京|   中国|     M|   2003-02-24|     NULL|2003|\n",
      "|     601028|  周倩|    周倩@fake_yahoo.com|15655389611|香港|   中国|     M|   2003-06-25|     NULL|2003|\n",
      "|     435571|  樊强|    樊强@fake_gmail.com|13250163737|北京|   中国|     F|   2003-03-12|     NULL|2003|\n",
      "|     601030|樊建华|樊建华@fake_hotmail.com|15671705029|香港|   中国|     M|   2003-11-28|     NULL|2003|\n",
      "|     435573|  张莉|  张莉@fake_hotmail.com|13900727794|北京|   中国|     F|   2003-01-09|     NULL|2003|\n",
      "|     601044|  宁琴|    宁琴@fake_yahoo.com|15648081550|香港|   中国|     M|   2003-06-27|     NULL|2003|\n",
      "|     435574|马凤兰|马凤兰@fake_hotmail.com|18926113401|北京|   中国|     M|   2003-05-30|     NULL|2003|\n",
      "|     601045|  祁旭|  祁旭@fake_hotmail.com|18132504216|香港|   中国|     M|   2003-04-03|     NULL|2003|\n",
      "|     435579|  张萍|    张萍@fake_gmail.com|18807124646|北京|   中国|     M|   2003-08-31|     NULL|2003|\n",
      "+-----------+------+-----------------------+-----------+----+-------+------+-------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\lib\\site-packages\\pyspark\\sql\\session.py:1796\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[1;32m-> 1796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[0;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\lib\\site-packages\\pyspark\\context.py:654\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[0;32m    657\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    661\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mc:\\Users\\user\\miniconda3\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
